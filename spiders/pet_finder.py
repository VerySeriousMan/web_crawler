# -*- coding: utf-8 -*-
"""
Project Name: web_crawler
File Created: 2025.05.16
Author: ZhangYuetao
File Name: pet_finder.py
Update: 2025.06.20
"""

import os
import time

import requests
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

import utils
import config
from logger import logger

IMAGE_IDX_LOCK = threading.Lock()        # ç”¨äºä¿æŠ¤ IMAGE_IDX çš„ä¿®æ”¹
USED_URLS_LOCK = threading.Lock()        # ç”¨äºä¿æŠ¤ USED_IMAGE_URLS çš„è®¿é—®
FILE_LOCKS = {                           # ç”¨äºä¿æŠ¤å†™æ–‡ä»¶çš„æ“ä½œ
    'success': threading.Lock(),
    'fail': threading.Lock(),
}

WEB_NAME = 'petfinder'

basic_setting = config.load_config(config.BASIC_SETTING_PATH, config.BASIC_SETTING_DEFAULT_CONFIG)
save_path = basic_setting["save_path"]
type_name = basic_setting["type_name"]

IMAGE_IDX = config.get_idx(WEB_NAME, type_name, 'images')

used_images = config.init_used_urls(config.get_save_history_path(WEB_NAME, type_name, 'used', 'images'))
wrong_images = config.init_used_urls(config.get_save_history_path(WEB_NAME, type_name, 'wrong', 'images'))
USED_IMAGE_URLS = used_images.update(wrong_images)
used_pages = config.init_used_urls(config.get_save_history_path(WEB_NAME, type_name, 'used', 'pages'))
wrong_pages = config.init_used_urls(config.get_save_history_path(WEB_NAME, type_name, 'wrong', 'pages'))
USED_PAGE_URLS = used_pages.update(wrong_pages)

utils.begin_logger(WEB_NAME, save_path, type_name, 
                   image_idx=IMAGE_IDX, 
                   used_image_urls=USED_IMAGE_URLS, used_page_urls=USED_PAGE_URLS)


def add_idx():
    """
    å¢åŠ å›¾ç‰‡ç´¢å¼•ã€‚
    """
    global IMAGE_IDX
    with IMAGE_IDX_LOCK:
        IMAGE_IDX += 1


def search_pet_pages(max_pages=10, random_proxy=False, random_user_agent=False, headless=False, use_open_chrome=False, need_load=False):
    """
    æœç´¢ PetFinder é¡µé¢é“¾æ¥ã€‚
    
    :param max_pages: æœ€å¤§æœç´¢é¡µæ•°ã€‚
    :param random_proxy: æ˜¯å¦ä½¿ç”¨éšæœºä»£ç†ã€‚
    :param random_user_agent: æ˜¯å¦ä½¿ç”¨éšæœº User-Agentã€‚
    :param headless: æ˜¯å¦å¯ç”¨æ— å¤´æ¨¡å¼ã€‚
    :param use_open_chrome: æ˜¯å¦ä½¿ç”¨å·²æ‰“å¼€çš„ Chrome æµè§ˆå™¨ã€‚
    :param need_load: æ˜¯å¦éœ€è¦æ‰‹åŠ¨ç™»å½•é¡µé¢ã€‚
    :return: æœç´¢åˆ°çš„é¡µé¢é“¾æ¥åˆ—è¡¨ã€‚
    """
    options = utils.create_option(random_proxy, random_user_agent, headless, use_open_chrome)
    
    driver = webdriver.Chrome(options=options)

    all_urls = set()
    
    if need_load:
        logger.info("éœ€è¦æ‰‹åŠ¨ç™»å½•é¡µé¢")
        # åŠ è½½ç™»å½•é¡µé¢
        driver.get('https://www.petfinder.com/search/cats-for-adoption/us/ca/los-angeles/?distance=Anywhere')
        
        # ç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨ç™»å½•
        print("è¯·åœ¨æµè§ˆå™¨ä¸­æ‰‹åŠ¨ç™»å½•ï¼Œç™»å½•å®ŒæˆåæŒ‰ Enter é”®ç»§ç»­...")
        input()
        
        logger.info("ç™»å½•å®Œæˆ,å¼€å§‹æœç´¢")

    for page in range(max_pages):
        logger.info(f"begin page: [{page}]")
        # åŠ è½½æœç´¢é¡µé¢
        time.sleep(5)
        retries = 0
        max_retries = 3

        while retries < max_retries:
            try:
                page_links = WebDriverWait(driver, 10).until(
                    EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'a.petCard-link')))

                logger.info(f"get {len(page_links)} urls")
                for page_link in page_links:
                    page_url = page_link.get_attribute('href')

                    if page_url and 'www.petfinder.com/cat' in page_url:
                        all_urls.add(page_url)

                next_button = WebDriverWait(driver, 10).until(
                    EC.element_to_be_clickable((
                        By.CSS_SELECTOR,
                        'button.fieldBtn.fieldBtn_altHover.m-fieldBtn_iconRt.m-fieldBtn_tight.m-fieldBtn_full')))

                next_button.click()
                break
            except Exception as e:
                retries += 1  # å¤±è´¥åå¢åŠ é‡è¯•æ¬¡æ•°
                if retries == max_retries:
                    logger.error(f"é‡è¯•æ¬¡æ•°è¾¾åˆ°æœ€å¤§å€¼ï¼ŒæŸ¥æ‰¾å¤±è´¥:{e}ã€‚")
                    
                logger.error(f"å¤±è´¥ï¼Œè¿›è¡Œé‡è¯•ï¼Œé‡è¯•æ¬¡æ•°ï¼š{retries}/{max_retries}ï¼Œé”™è¯¯ä¿¡æ¯ï¼š{e}")
                driver.refresh()
                time.sleep(10)
                
        if retries == max_retries:
            logger.error(f"ææ—©é€€å‡ºï¼š{page}")
            break

    all_urls = list(all_urls)
    utils.save_list_to_txt(config.get_save_history_path(WEB_NAME, type_name, 'get', 'pages'), all_urls)
    logger.info(f"è·å–åˆ° {len(all_urls)} æ¡ URLã€‚")

    return all_urls


def fetch_images_from_page(page_url):
    """
    ä»å•ä¸ªé¡µé¢è·å–å›¾ç‰‡é“¾æ¥ã€‚
    
    :param page_url: é¡µé¢ URLã€‚
    :return: å›¾ç‰‡é“¾æ¥åˆ—è¡¨æˆ–å¤±è´¥çš„ URLã€‚
    """
    if page_url in USED_PAGE_URLS:
        logger.info(f"é“¾æ¥ {page_url} å·²ç»å¤„ç†è¿‡ï¼Œè·³è¿‡ä¸‹è½½ã€‚")
        return [], None
        
    proxies = {
        'http': 'http://127.0.0.1:2081',
        'https': 'http://127.0.0.1:2081',
    }
    pet_id = page_url.split('/')[4].split('-')[-1]

    for i in range(3):
        header = {
            'User-Agent': utils.get_random_user_agent(),
        }

        try:
            response = requests.get(page_url, headers=header, proxies=proxies, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, 'html.parser')
            image_tags = soup.find_all('img', class_='petCarousel-body-slide')

            urls = [(pet_id, img.get('src')) for img in image_tags if img.get('src')]
            USED_PAGE_URLS.add(page_url)
            logger.info(f"âœ… æˆåŠŸè·å–å›¾ç‰‡ from {page_url} â€” å…± {len(urls)} å¼ ")
            
            return urls, None
        
        except Exception as e:
            logger.error(f"âŒ è·å–å¤±è´¥ ({page_url}), é‡è¯•ç¬¬ {i + 1} æ¬¡: {e}")

    return [], page_url  # è¿”å›ç©ºå›¾åƒå’Œå¤±è´¥çš„ URL


def get_images_in_pages(page_urls, max_workers=5):
    """
    ä»é¡µé¢åˆ—è¡¨ä¸­è·å–å…¨éƒ¨å›¾åƒé“¾æ¥ã€‚
    
    :param page_urls: é¡µé¢ URL åˆ—è¡¨ã€‚
    :param max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°ã€‚
    :return: å›¾åƒé“¾æ¥åˆ—è¡¨ã€‚
    """
    all_urls = []
    wrong_urls = []
    lock = threading.Lock()

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_url = {executor.submit(fetch_images_from_page, url): url for url in page_urls}

        for i, future in enumerate(as_completed(future_to_url), 1):
            urls, wrong_url = future.result()

            with lock:
                all_urls.extend(urls)  # å°†å›¾åƒ URL æ·»åŠ åˆ°åˆ—è¡¨
                if wrong_url:
                    wrong_urls.append(wrong_url)  # å°†å¤±è´¥çš„ URL æ·»åŠ åˆ°åˆ—è¡¨

                if i % 100 == 0:
                    logger.info(f"âš™ï¸ å·²å¤„ç† {i} ä¸ªé¡µé¢ï¼Œä¿å­˜ä¸­...")
                    utils.save_list_to_txt(config.get_save_history_path(WEB_NAME, type_name, 'get', 'images'), all_urls)
                    utils.save_list_to_txt(config.get_save_history_path(WEB_NAME, type_name, 'wrong', 'pages'), wrong_urls)
                    all_urls.clear()
                    wrong_urls.clear()

    utils.save_list_to_txt(config.get_save_history_path(WEB_NAME, type_name, 'get', 'images'), all_urls)
    utils.save_list_to_txt(config.get_save_history_path(WEB_NAME, type_name, 'wrong', 'pages'), wrong_urls)
    logger.info(f"ğŸ‰ å…¨éƒ¨å®Œæˆï¼Œå…±å¤„ç† {len(page_urls)} ä¸ªé¡µé¢ã€‚")

    return all_urls


def download_images_thread(image_infos, save_dir, max_workers=8):
    """
    å¤šçº¿ç¨‹ä¸‹è½½å›¾åƒã€‚
    
    :param image_infos: å›¾åƒä¿¡æ¯åˆ—è¡¨ã€‚
    :param save_dir: ä¿å­˜ç›®å½•ã€‚
    :param max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°ã€‚
    """
    os.makedirs(save_dir, exist_ok=True)
    if not image_infos:
        logger.warning('urls empty')
        return

    old_idx = IMAGE_IDX

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(download_image, info, save_dir) for info in image_infos]
        for future in as_completed(futures):
            future.result()
            
    config.update_idx(WEB_NAME, type_name, 'images', IMAGE_IDX)
    
    logger.info(f"ä¸‹è½½å…¨éƒ¨å®Œæˆï¼Œæœ¬æ¬¡å…±ä¸‹è½½{IMAGE_IDX - old_idx}ä¸ªè§†é¢‘ï¼ˆæ–°è®¡æ•°å€¼ï¼ŒIMAGE_IDX={IMAGE_IDX})")


def download_images(image_infos, save_dir):
    """
    å•çº¿ç¨‹ä¸‹è½½å›¾åƒã€‚
    
    :param image_infos: å›¾åƒä¿¡æ¯åˆ—è¡¨ã€‚
    :param save_dir: ä¿å­˜ç›®å½•ã€‚
    """
    os.makedirs(save_dir, exist_ok=True)
    if not image_infos:
        logger.warning('urls empty')
        return

    old_idx = IMAGE_IDX

    for image_info in image_infos:
        download_image(image_info, save_dir)
    
    config.update_idx(WEB_NAME, type_name, 'images', IMAGE_IDX)

    logger.info(f"ä¸‹è½½å…¨éƒ¨å®Œæˆï¼Œæœ¬æ¬¡å…±ä¸‹è½½{IMAGE_IDX - old_idx}ä¸ªè§†é¢‘ï¼ˆæ–°è®¡æ•°å€¼ï¼ŒIMAGE_IDX={IMAGE_IDX})")


def download_image(image_info, save_dir):
    """
    ä¸‹è½½å›¾åƒã€‚
    
    :param image_info: å›¾åƒä¿¡æ¯ã€‚
    :param save_dir: ä¿å­˜ç›®å½•ã€‚
    """
    if not image_info:
        logger.info('emptyï¼')
        return

    image_id = image_info[0]
    image_url = image_info[1]

    with USED_URLS_LOCK:
        if image_url in USED_IMAGE_URLS:
            logger.info(f"å›¾ç‰‡é“¾æ¥ {image_url} å·²ç»å¤„ç†è¿‡ï¼Œè·³è¿‡ä¸‹è½½ã€‚")
            return

    # å‘é€ GET è¯·æ±‚è·å–è§†é¢‘å†…å®¹
    header = {
        'User-Agent': utils.get_random_user_agent(),
    }

    wrong_url = None

    for _ in range(3):
        proxies = {
            'http': 'http://127.0.0.1:2081',
            'https': 'http://127.0.0.1:2081',
        }
        try:
            response = requests.get(image_url, headers=header, proxies=proxies, timeout=10)
            response.raise_for_status()  # å¦‚æœè¯·æ±‚å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
            image_data = response.content

            current_time = utils.get_formatted_timestamp()

            filename = f'{type_name[0].upper()}_pf{image_id}_{current_time}_RGB.jpg'
            dir_name = f'pf{image_id}'

            save_path = os.path.join(save_dir, dir_name, filename)
            os.makedirs(os.path.join(save_dir, dir_name), exist_ok=True)

            with open(save_path, 'wb') as file:
                file.write(image_data)

            with FILE_LOCKS['success']:
                utils.save_list_to_txt(config.get_save_history_path(WEB_NAME, type_name, 'used', 'images'), [image_url])

            with USED_URLS_LOCK:
                USED_IMAGE_URLS.add(image_url)

            add_idx()

            logger.info(f"å›¾ç‰‡å·²æˆåŠŸä¸‹è½½åˆ° {save_path}")
            wrong_url = None
            break
        except requests.exceptions.RequestException as e:
            logger.error(f"ä¸‹è½½å¤±è´¥: {e}")
            wrong_url = image_url
            continue

    if wrong_url:
        with FILE_LOCKS['fail']:
            utils.save_list_to_txt(config.get_save_history_path(WEB_NAME, type_name, 'wrong', 'images'), [wrong_url])


def run(save_dir, max_page=10, max_search_workers=1, max_download_workers=1, random_proxy=False, random_user_agent=False, 
        headless=False, use_open_chrome=False, need_load=True, have_pages=False, have_urls=False):
    """
    ç»Ÿä¸€å…¥å£å‡½æ•°ï¼Œä¾›è°ƒåº¦å™¨è°ƒç”¨ã€‚
    
    :param save_dir: ä¿å­˜è·¯å¾„ã€‚
    :param max_page: æœ€å¤§çˆ¬å–é¡µé¢æ•°é‡ã€‚
    :param max_search_workers: æœ€å¤§æœç´¢çº¿ç¨‹æ•°ã€‚
    :param max_download_workers: æœ€å¤§ä¸‹è½½çº¿ç¨‹æ•°ã€‚
    :param random_proxy: æ˜¯å¦ä½¿ç”¨éšæœºä»£ç†ã€‚
    :param random_user_agent: æ˜¯å¦ä½¿ç”¨éšæœºUser-Agentã€‚
    :param headless: æ˜¯å¦å¯ç”¨æ— å¤´æ¨¡å¼ã€‚
    :param use_open_chrome: æ˜¯å¦ä½¿ç”¨å·²æ‰“å¼€çš„Chromeæµè§ˆå™¨ã€‚
    :param need_load: æ˜¯å¦éœ€è¦æ‰‹åŠ¨ç™»å½•é¡µé¢ã€‚
    :param have_pages: æ˜¯å¦å·²ç»è·å–è¿‡é¡µé¢é“¾æ¥ã€‚
    :param have_urls: æ˜¯å¦å·²ç»è·å–è¿‡å›¾ç‰‡é“¾æ¥ã€‚
    """
    os.makedirs(save_dir, exist_ok=True)
    
    old_image_idx = IMAGE_IDX

    logger.info(f'========== å¼€å§‹å¤„ç†: {type_name} ==========')
    try:
        if have_pages:
            pages = utils.read_list_from_txt(config.get_save_history_path(WEB_NAME, type_name, 'get', 'pages'))
        else:
            pages = search_pet_pages(max_page, random_proxy, random_user_agent, headless, use_open_chrome, need_load)
            
        if not pages:
            logger.warning(f"[{type_name}]æœªè·å–åˆ°æœ‰æ•ˆé¡µé¢é“¾æ¥ï¼Œæå‰é€€å‡ºã€‚")
            return
        
        if have_urls:
            image_urls = utils.read_list_from_txt(config.get_save_history_path(WEB_NAME, type_name, 'get', 'images'))
        else:
            image_urls = get_images_in_pages(pages, max_search_workers)
        
        if not image_urls:
            logger.warning(f"[{type_name}]æœªæå–åˆ°æœ‰æ•ˆèµ„æºï¼Œæå‰é€€å‡ºã€‚")
            return
        
        if max_download_workers == 1:
            download_images(image_urls, save_dir)
        else:
            download_images_thread(image_urls, save_dir, max_download_workers)
    except Exception as e:
        logger.error(f"[{type_name}] æŠ“å–å¤±è´¥: {e}")
    logger.info(f'========== å®Œæˆ: {type_name} ==========')
    
    utils.end_logger(WEB_NAME, save_dir, type_name, 
                     image_idx=IMAGE_IDX,
                     new_image_count=IMAGE_IDX-old_image_idx)
        